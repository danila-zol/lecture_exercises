{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import pairwise\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 23:41:59.776881: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-26 23:42:00.615599: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-26 23:42:00.615975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-26 23:42:00.776956: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-26 23:42:01.068566: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-26 23:42:04.743057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, Concatenate, Lambda\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "snapshot_folder = './chatbot_weights'\n",
    "\n",
    "GRU_units = 256\n",
    "batch_size = 32\n",
    "emb_dim = 50\n",
    "\n",
    "init_lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель на TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейросеть на Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я буду использовать датасет разговоров людей с моделями отыгрывающими выдуманных персонажей на сайте character.ai .   \n",
    "Ссылка на датасет: https://huggingface.co/datasets/PygmalionAI/PIPPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала загрузим датасет и сформируем датафрэйм с парами вопрос-ответ на котором будет проводиться обучение. Также сразу токенизируми и очистим текст от цифр, пунктуации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"./pippa_deduped.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обучать чатбот под конкретного персонажа, но мы будем использовать весь датасет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако чтобы сократить время обработки и обучения мы ограничемся только 5% доступного датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversations, dummy = train_test_split(data[\"conversation\"], train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, incomplete='ignore', fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    if incomplete == 'fill':\n",
    "        return zip_longest(*args, fillvalue=fillvalue)\n",
    "    if incomplete == 'strict':\n",
    "        return _zip_equal(*args)\n",
    "    if incomplete == 'ignore':\n",
    "        return zip(*args)\n",
    "    else:\n",
    "        raise ValueError('Expected fill, strict, or ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(t):\n",
    "    \n",
    "    tokenized_text = [\n",
    "        w for w in word_tokenize(t.lower())\n",
    "        if w.isalpha()\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_pairs = []\n",
    "start_token = '<startseq>'\n",
    "end_token = '<endseq>'\n",
    "for conversation in train_conversations:\n",
    "    messages = [start_token + \" \" + text_clean(x[\"message\"]) + \" \" + end_token for x in conversation]\n",
    "    response_pairs.extend(pairwise(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это дает нам около 6 тычяч пар вопрос-ответ, что более чем достаточно для обучения простого чатбота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406572"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглядят обработанные входные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<startseq> today you decided to go to his house when you got to his house you knocked on his door but there was no answer at all which makes you decided immediately to enter his house and head into his room when you arrive in his room unfortunately he is not there but not long after when you are still pacing in his room he appears behind you and approaches you he pulls your hand making you facing him you really need to be taught a lesson he said menacingly before he smirks at you <endseq>',\n",
       "  '<startseq> she looks at him but i do need a lesson she says in playfull tone <endseq>'),\n",
       " ('<startseq> she looks at him but i do need a lesson she says in playfull tone <endseq>',\n",
       "  '<startseq> you look down on me i can see it in your eyes but don t you forget i am the leader of the uchiha clan he said firmly while he puts his hand on your chin you can feel his fingers through his gloves you begin to blush deeply when he does so <endseq>'),\n",
       " ('<startseq> you look down on me i can see it in your eyes but don t you forget i am the leader of the uchiha clan he said firmly while he puts his hand on your chin you can feel his fingers through his gloves you begin to blush deeply when he does so <endseq>',\n",
       "  '<startseq> she blushes deeply and looks away because of that her sudden change makes madara want to laughs <endseq>'),\n",
       " ('<startseq> she blushes deeply and looks away because of that her sudden change makes madara want to laughs <endseq>',\n",
       "  '<startseq> are you shy how adorable i didn t think i can make you blush he laughs loudly at you you feel insulted a little i thought that someone who is brave enough who can break a door to enter into my house won t blush so easily he said while he is still laughing at you but not in a mean way <endseq>'),\n",
       " ('<startseq> are you shy how adorable i didn t think i can make you blush he laughs loudly at you you feel insulted a little i thought that someone who is brave enough who can break a door to enter into my house won t blush so easily he said while he is still laughing at you but not in a mean way <endseq>',\n",
       "  '<startseq> after he says that she looks at him still blushing then she get closer to him and suddenly she kisses him on the lips gently but sensually <endseq>')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile 80 of len of questions: 68.0\n",
      "longest sentence:  1439\n",
      "\n",
      "percentile 80 of len of answers: 68.0\n",
      "longest sentence:  1439\n",
      "\n",
      "max-len questions for training:  68\n",
      "max-len answers for training:  68\n"
     ]
    }
   ],
   "source": [
    "def max_length(pairs,prct):\n",
    "    # Create a list of all the captions\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for i in pairs:\n",
    "        questions.append(i[0])\n",
    "        answers.append(i[1])\n",
    "        \n",
    "    length_questions = list(len(d.split()) for d in questions)\n",
    "    length_answers = list(len(d.split()) for d in answers)\n",
    "\n",
    "    print('percentile {} of len of questions: {}'.format(prct,np.percentile(length_questions, prct)))\n",
    "    print('longest sentence: ', max(length_questions))\n",
    "    print()\n",
    "    print('percentile {} of len of answers: {}'.format(prct,np.percentile(length_answers, prct)))\n",
    "    print('longest sentence: ', max(length_answers))\n",
    "    print()\n",
    "    return int(np.percentile(length_questions, prct)),int(np.percentile(length_answers, prct))\n",
    "\n",
    "max_len_q,max_len_a = max_length(response_pairs,80)\n",
    "\n",
    "print('max-len questions for training: ', max_len_q)\n",
    "print('max-len answers for training: ', max_len_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short vocab size: 28951 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<endseq>', '<startseq>', 'a', 'aa', 'aaa']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a vocabulary of the words that occur more than word_count_threshold time\n",
    "def create_reoccurring_vocab(pairs, word_count_threshold = 5):\n",
    "    p = pairs\n",
    "    # Create a list of all the captions\n",
    "    all_captions = []\n",
    "    for i in p:\n",
    "        for j in i:\n",
    "            all_captions.append(j)\n",
    "\n",
    "    # Consider only words which occur at least 10 times in the corpus\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    vocab = list(set(vocab))\n",
    "    print('Short vocab size: %d ' % len(vocab))\n",
    "    return vocab\n",
    "\n",
    "# each word in the vocabulary must be used in the data atleast 20 times\n",
    "short_vocab = create_reoccurring_vocab(response_pairs, word_count_threshold = 10)\n",
    "\n",
    "\n",
    "short_vocab = sorted(short_vocab)[1:]\n",
    "short_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28951"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len = len(short_vocab) + 1 # since index 0 is used as padding, we have to increase the vocab size\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trimmed from 406572 pairs to 322073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "322073"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the pairs that have the words in vocab\n",
    "def trimRareWords(voc, pairs):\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    i=0\n",
    "    for pair in pairs:\n",
    "        i+=1\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"\\nTrimmed from {} pairs to {}\".format(len(pairs), len(keep_pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# # Trim voc and pairs\n",
    "pairs_final = trimRareWords(short_vocab, response_pairs)\n",
    "with open ('final_pairs_v21.pkl','wb') as f:\n",
    "    pairs_final = pickle.dump(pairs_final,f)\n",
    "    \n",
    "with open ('final_pairs_v21.pkl','rb') as f:\n",
    "    pairs_final = pickle.load(f)\n",
    "    \n",
    "pairs_final_train = pairs_final\n",
    "len(pairs_final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the tokenizer object:\n",
    "tokenizer = Tokenizer(filters = [])\n",
    "tokenizer.fit_on_texts(short_vocab)\n",
    "\n",
    "ixtoword = {} # index to word dic\n",
    "wordtoix = tokenizer.word_index # word to index dic\n",
    "pad_token = 'pad0'\n",
    "ixtoword[0] = pad_token # no word in vocab has index 0. but padding is indicated with 0\n",
    "\n",
    "for w in tokenizer.word_index:\n",
    "    ixtoword[tokenizer.word_index[w]] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove...\n",
      "GloVe  50  loded.\n"
     ]
    }
   ],
   "source": [
    "# Making the embedding mtrix\n",
    "def make_embedding_layer(embedding_dim=50, glove=True):\n",
    "    print('Loading glove...')\n",
    "    glove_dir = './glove'\n",
    "    embeddings_index = {} \n",
    "    f = open(os.path.join(glove_dir, 'glove.6B.'+str(embedding_dim)+'d.txt'), encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print(\"GloVe \",embedding_dim, ' loded.')\n",
    "    # Get 200-dim dense vector for each of the vocab_rocc\n",
    "    embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
    "    for word, i in wordtoix.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in the embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we \n",
    "                                                                                           # do not train the embedding layer\n",
    "                                                                                           # we use 0 as padding so => mask_zero=True\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([embedding_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "embeddings = make_embedding_layer(embedding_dim=emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.Bidirectional1 = Bidirectional(GRU(enc_units, \n",
    "                                                return_sequences=True, \n",
    "                                                return_state=True,\n",
    "                                                recurrent_initializer='glorot_uniform',\n",
    "                                                name='gru_1'), name='bidirectional_encoder1')\n",
    "                                                                                                \n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.Inp = Input(shape=(max_len_q,)) # size of questions\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        output, state_f, state_b = self.Bidirectional1(x)\n",
    "\n",
    "        return output, state_f, state_b\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(emb_dim, GRU_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.units = units\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        \n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_size\n",
    "        self.embeddings = embeddings\n",
    "        self.units = 2 * dec_units # because we use bidirectional encoder\n",
    "        self.fc = Dense(vocab_len, activation='softmax', name='dense_layer')\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        self.decoder_gru_l1 = GRU(self.units, return_sequences=True, \n",
    "                                  return_state= False, recurrent_initializer='glorot_uniform' ,name='decoder_gru1')\n",
    "        self.decoder_gru_l2 = GRU(self.units, return_sequences=False, \n",
    "                                  return_state= True, recurrent_initializer='glorot_uniform' ,name='decoder_gru2') \n",
    "        self.dropout = Dropout(0.4)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # concat input and context vector together\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        x = self.decoder_gru_l1(x)\n",
    "        x = self.dropout(x)\n",
    "        output, state = self.decoder_gru_l2(x)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_len, emb_dim, GRU_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    \n",
    "    attention_plot = np.zeros((max_len_a, max_len_q))\n",
    "\n",
    "    sentence = unicode_to_ascii(sentence.lower())\n",
    "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
    "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
    "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, GRU_units))]\n",
    "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "    dec_input = tf.expand_dims([wordtoix[start_token]], 1)\n",
    "\n",
    "    for t in range(max_len_a):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = K.get_value(attention_weights)\n",
    "        \n",
    "        predicted_id =  K.get_value(tf.argmax(predictions[0]))       \n",
    "\n",
    "        if ixtoword[predicted_id] == end_token:\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        result += ixtoword[predicted_id] + ' '\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(sentence, training=False):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    if training:\n",
    "        return result\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted answer: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(sentence, k=3, maxsample=max_len_a, use_unk=False, oov=None, eos=end_token):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    \n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [[wordtoix[start_token]]]\n",
    "    live_scores = [0]\n",
    "\n",
    "    sentence = unicode_to_ascii(sentence.lower())\n",
    "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
    "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
    "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    hidden = [tf.zeros((1, GRU_units))]\n",
    "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "    dec_input = tf.expand_dims([wordtoix[start_token]], 0)\n",
    "        \n",
    "    while live_k and dead_k < k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
    "        probs = K.get_value(predictions[0])\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        cand_flat = cand_scores.flatten()\n",
    "\n",
    "        # find the best (lowest) scores we have from all possible samples and new words\n",
    "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "        live_scores = cand_flat[ranks_flat]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = vocab_len\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    final_samples = dead_samples + live_samples\n",
    "    final_scores = dead_scores + live_scores   \n",
    "    \n",
    "    # cutting the strong where end_token is encounterd\n",
    "    for i in range(len(final_scores)):\n",
    "        final_scores[i] /= len(final_samples[i]) # normalizing the scores\n",
    "    \n",
    "    final_result =[]\n",
    "    \n",
    "    for i in range(len(final_scores)):\n",
    "        final_result.append((final_scores[i],final_samples[i]))\n",
    "    \n",
    "    final_list_ix = max(final_result)[1]\n",
    "    final_list_word = [ixtoword[f] for f in final_list_ix]\n",
    "    final_sentence = ' '.join(final_list_word[1:])\n",
    "    end_ix = final_sentence.find(end_token)\n",
    "    return final_sentence[:end_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(init_lr)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = K.sparse_categorical_crossentropy(real, pred, from_logits= False)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(snapshot_folder, str(emb_dim)+\"-ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden_f, enc_hidden_b = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
    "        dec_input = tf.expand_dims([wordtoix[start_token]] * batch_size, 1) # dec_input initially == start_token\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            \n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions) # each time just use one timestep output\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1) # expected output at this time becomes input for next timestep\n",
    "            \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "history={'loss':[]}\n",
    "smallest_loss = np.inf\n",
    "best_ep = 1\n",
    "EPOCHS = 157\n",
    "enc_hidden = encoder.initialize_hidden_state()\n",
    "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
    "current_ep = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bot(k = 3, beam = False):\n",
    "    print('#'*20)\n",
    "    q = 'Hello'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'How are you'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q= 'Are you my friend'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'What are you doing'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'Who are you'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('%')\n",
    "    q = 'Do you want to go out'\n",
    "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
    "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
    "    print('#'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history():\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(best_ep,smallest_loss,'ro')\n",
    "    plt.plot(history['loss'],'b-')\n",
    "    plt.legend(['best','loss'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_loss = K.constant(0)\n",
    "X, y = [], []\n",
    "for ep in range(current_ep,EPOCHS):\n",
    "    current_ep = ep    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    btch = 1\n",
    "\n",
    "    for p in pairs_final_train:     \n",
    "        \n",
    "        question = p[0]\n",
    "        label = p[1]\n",
    "        # find the index of each word of the caption in vocabulary\n",
    "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
    "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
    "        # encoder input and decoder input and label\n",
    "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
    "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
    "        \n",
    "        X.append(enc_in_seq)\n",
    "        y.append(dec_out_seq)\n",
    "\n",
    "        if len(X) == batch_size :\n",
    "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "            X , y = [], []\n",
    "            btch += 1\n",
    "            if btch % (steps_per_epoch//6) == 0:\n",
    "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep , btch, K.get_value(batch_loss)))\n",
    "\n",
    "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
    "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep ,epoch_loss))\n",
    "    history['loss'].append(epoch_loss)\n",
    "    \n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "#    test_bot(k=5)\n",
    "\n",
    "    if epoch_loss < smallest_loss:\n",
    "        smallest_loss = epoch_loss\n",
    "        best_ep = ep \n",
    "        print('check point saved!')\n",
    "    \n",
    "    if ep % 3 == 0:\n",
    "        plot_history()\n",
    "        \n",
    "    print('Best epoch so far: ',best_ep,' smallest loss:',smallest_loss)\n",
    "    print('Time taken for the epoch {:.3f} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    print('=' * 40)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
